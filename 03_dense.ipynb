{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('./datasets/v0_eigens.npz')\n",
    "\n",
    "train_data_size = dataset['train_eigens'].shape[0]\n",
    "valid_data_size = train_data_size / 5\n",
    "train_data_size = train_data_size - valid_data_size\n",
    "\n",
    "indices = np.arange(train_data_size + valid_data_size)\n",
    "\n",
    "train_data = dataset['train_eigens'][indices[:train_data_size]]\n",
    "valid_data = dataset['train_eigens'][indices[train_data_size:]]\n",
    "\n",
    "train_eigens = train_data[:, :-28]\n",
    "train_labels = train_data[:, -28:]\n",
    "valid_eigens = valid_data[:, :-28]\n",
    "valid_labels = valid_data[:, -28:]\n",
    "issue_eigens = dataset['issue_eigens'][:, :-28]\n",
    "\n",
    "print 'train_eigens.shape = {}'.format(train_eigens.shape)\n",
    "print 'train_labels.shape = {}'.format(train_labels.shape)\n",
    "print 'valid_eigens.shape = {}'.format(valid_eigens.shape)\n",
    "print 'valid_labels.shape = {}'.format(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Keras Model\n",
    "\n",
    "* **Dense** : it is a kind of layer in neural network. actually, it's the w & b in the previous session.\n",
    "* **relu** : we call it \"activation function\". an activation function decide how a neuron pass through a layer. \"rulu\" change the tensors so that all neurons that less than zero become zero.\n",
    "* **sigmoid** : sigmoid is another activation function. it re-map all values to the range between 0.0~1.0. we usually use sigmoid to represent 'probability' or 'confidence'.\n",
    "* **optimizer** : an optimizer that update variables (w & b).\n",
    "* **loss** : a function define the loss (our objective).\n",
    "\n",
    "https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # NOTE: shape of input tensors is (N, 896)\n",
    "    # TODO: change number of neurons of each layer\n",
    "    model.add(Dense(64, activation='relu', input_shape=(896,)))\n",
    "\n",
    "    # TODO: change the activation of hidden layers and check the performance\n",
    "    #       possible value: 'elu' / 'selu' / 'tanh' / 'relu' / 'sigmoid' / ...\n",
    "    \n",
    "    # TODO: add more dense layer and check the performance\n",
    "    \n",
    "    # NOTE: the number of neurons of the final layer must be 28 cause we are predicting 28 values.\n",
    "    model.add(Dense(28, activation='sigmoid'))\n",
    "\n",
    "    # TODO: change the optimizer and check the performance\n",
    "    #       possible values: 'sgd' / 'adam' / ...\n",
    "\n",
    "    # TODO: change the loss function and check the performance\n",
    "    #       possible values: 'mean_squared_error' / 'binary_crossentropy' / ...\n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# TODO: change 'batch_size' and check the peformance\n",
    "\n",
    "# TODO: change 'epochs' and check the peformance\n",
    "\n",
    "model.fit(\n",
    "    x=train_eigens,\n",
    "    y=train_labels,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=2,\n",
    "    validation_data=(valid_eigens, valid_labels),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate\n",
    "\n",
    "Now we have a trained model, let's validate the performance on validation dataset. If the performance is good, we can use the model to predict labels of testing dataset.\n",
    "\n",
    "Known Best: **0.8715065402653539**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_guesss = model.predict(valid_eigens)\n",
    "\n",
    "auc = util.auc(valid_guesss, valid_labels)\n",
    "\n",
    "print auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_guesss = model.predict(issue_eigens)\n",
    "\n",
    "util.write_result('dense.csv', issue_guesss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
