{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Dataset\n",
    "\n",
    "- generate 128 dots around a stright line.\n",
    "- plot the fake dataset\n",
    "- we know $w$ and $b$, but we pretend we know nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secret_dots():\n",
    "    \"\"\"\n",
    "    generate 128 dots around y = w * x + b\n",
    "    \"\"\"\n",
    "    w = 0.9\n",
    "    b = 0.2\n",
    "\n",
    "    xs = np.random.uniform(low=0.0, high=5.0, size=(128, 1))\n",
    "    ys = xs * w + b + np.random.uniform(low=-0.05, high=0.05, size=xs.shape)\n",
    "    \n",
    "    return xs, ys\n",
    "\n",
    "eigens, labels = secret_dots()\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 10), dpi=50)\n",
    "plt.title('y = wx + b')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(eigens, labels, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Function\n",
    "\n",
    "![](./assets/001_loss.png)\n",
    "\n",
    "We now have some dots (x = eigens, y = labels).\n",
    "\n",
    "1. We **guess** all the dots fall on a stright line: $y = wx + b$. But we do not knwon $w, b$ yet.\n",
    "So we assign some values to $w, b$ randomly. Our goal is to find good $w, b$ so that all data falls on that line.\n",
    "\n",
    "2. We select a dot $(x_i, y_i)$ from the secret dataset.\n",
    "\n",
    "3. Appling $(x_i, y_i)$ to $y = wx + b$ gives us $z = wx_i + b$.\n",
    "\n",
    "4. Apparently, $z_i$ is far away from $y_i$ because $w, b$ are not **the chosen one**. We call the distance between $z_i$ & $y_i$ **loss**.\n",
    "\n",
    "What we want is to minimize the distance between $z_i$ and $y_i$.\n",
    "\n",
    "**We wish** : change $w$ and $b$ slightly so that $z_i$ is also changed slightly to **decrease the loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_w = 0.9\n",
    "secret_b = 0.2\n",
    "\n",
    "# a closer point introduces lower loss\n",
    "near_x = 3.0\n",
    "near_y = 3.0\n",
    "near_z = near_x * secret_w + secret_b\n",
    "near_loss = (near_z - near_y) ** 2\n",
    "\n",
    "# a further point introduces higher loss\n",
    "far_x = 1.0\n",
    "far_y = 4.0\n",
    "far_z = far_x * secret_w + secret_b\n",
    "far_loss = (far_z - far_y) ** 2\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 10), dpi=50)\n",
    "plt.plot(eigens, labels, '.')\n",
    "\n",
    "plt.plot(near_x, near_y, 'ro', label='loss[near]: {}'.format(near_loss))\n",
    "\n",
    "plt.plot(far_x, far_y, 'go', label='loss[far]: {}'.format(far_loss))\n",
    "\n",
    "plt.legend(loc='lower right', prop={'size': 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we do not know w & b, assign some random number to them\n",
    "w = 0.6\n",
    "b = 0.5\n",
    "\n",
    "# NOTE: chose a dot\n",
    "max_x = np.argmax(eigens)\n",
    "\n",
    "x1, y1 = eigens[max_x, 0], labels[max_x, 0]\n",
    "\n",
    "# NOTE: evaluate w and b to get z1\n",
    "z1 = w * x1 + b\n",
    "\n",
    "# NOTE: if z1 equals to y1, loss is 0 and we have nothing to improve.\n",
    "#       but z1 does not equal to y1 because w & b are random numbers.\n",
    "loss = (z1 - y1) ** 2\n",
    "\n",
    "# NOTE: generate some data to plot the loss function over w\n",
    "loss_ws = (np.arange(201, dtype=np.float) - 100.0) / 200.0 + 0.85\n",
    "loss_wl = (loss_ws * x1 + b - y1) ** 2\n",
    "\n",
    "# NOTE: generate some data to plot the loss function over b\n",
    "loss_bs = (np.arange(201, dtype=np.float) - 100.0) / 50.0 + 1.8\n",
    "loss_bl = (w * x1 + loss_bs - y1) ** 2\n",
    "\n",
    "# plot the loss function and loss of z1\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 10), dpi=50)\n",
    "\n",
    "# NOTE: plot loss v.s. w\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('w')\n",
    "plt.plot(loss_ws, loss_wl, '-')\n",
    "plt.plot(w, loss, 'ro')\n",
    "\n",
    "# NOTE: plot loss v.s. b\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('b')\n",
    "plt.plot(loss_bs, loss_bl, '-')\n",
    "plt.plot(b, loss, 'go')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathcal L = loss = (z_1 - y_1)^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "if we increase $z_1$ a little bit, what will happen to $\\mathcal L$ ?\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "slop(\\mathcal L, z_1) = \\frac{\\partial \\mathcal L}{\\partial z_1} = 2.0 \\times (z_1 - y_1)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (z1 - y1) ** 2\n",
    "\n",
    "# the slop at z1\n",
    "dl_dz = 2.0 * (z1 - y1)\n",
    "\n",
    "# NOTE: if we want to change loss, how much should we change w?\n",
    "dl_dw = dl_dz * x1\n",
    "\n",
    "# NOTE: if we want to change loss, how much should we change b?\n",
    "dl_db = dl_dz\n",
    "\n",
    "# NOTE: generate some data to plot the loss function over w\n",
    "loss_ws = (np.arange(201, dtype=np.float) - 100.0) / 200.0 + 0.85\n",
    "loss_wl = (loss_ws * x1 + b - y1) ** 2\n",
    "\n",
    "# NOTE: generate some data to plot the loss function over b\n",
    "loss_bs = (np.arange(201, dtype=np.float) - 100.0) / 50.0 + 1.8\n",
    "loss_bl = (w * x1 + loss_bs - y1) ** 2\n",
    "\n",
    "# plot the loss function and loss of z1\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 10), dpi=50)\n",
    "\n",
    "# NOTE: plot loss v.s. w\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('loss / w')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(loss_ws, loss_wl, '-')\n",
    "plt.plot(w, loss, 'ro')\n",
    "plt.arrow(w, loss, 0.1, 0.1 * dl_dw, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "\n",
    "# NOTE: plot loss v.s. b\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('loss / b')\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(loss_bs, loss_bl, '-')\n",
    "plt.plot(b, loss, 'go')\n",
    "plt.arrow(b, loss, 0.1, 0.1 * dl_db, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn W & b through Chain Rule\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "z_1 &= w \\times x_1 + b \\\\\n",
    "\\mathcal L &= (z_1 - y_1)^2 \\\\\n",
    "\\frac{\\partial z_1}{\\partial w} &= x_1 \\\\\n",
    "\\frac{\\partial z_1}{\\partial b} &= 1 \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial z_1} &= \\frac{\\partial {(z_1 - y_1)^2}}{\\partial z_1} = 2.0 \\times (z_1 - y_1) \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial w} &= \\frac{\\partial \\mathcal L}{\\partial z_1} \\frac{\\partial z_1}{\\partial w} = 2.0 \\times (z_1 - y_1) \\times x_1 \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial b} &= \\frac{\\partial \\mathcal L}{\\partial z_1} \\frac{\\partial z_1}{\\partial b} = 2.0 \\times (z_1 - y_1) \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "the slop points to the valley of loss, let's move $w, b$ to that direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_w, old_b = w, b\n",
    "\n",
    "# NOTE: what is the label if x is x1 wrt old_w & old_b?\n",
    "old_z1 = old_w * x1 + old_b\n",
    "\n",
    "# NOTE: loss\n",
    "old_loss = (old_z1 - y1) ** 2\n",
    "\n",
    "# NOTE: if we want to change loss, how much should we change z?\n",
    "dl_dz = 2.0 * (old_z1 - y1)\n",
    "\n",
    "# NOTE: if we want to change loss, how much should we change w?\n",
    "dl_dw = dl_dz * x1\n",
    "\n",
    "# NOTE: if we want to change loss, how much should we change b?\n",
    "dl_db = dl_dz\n",
    "\n",
    "# NOTE: update w & z\n",
    "#       why substract?\n",
    "#       change learning rate to see the difference\n",
    "learning_rate = 0.005\n",
    "\n",
    "new_w = old_w - learning_rate * dl_dw\n",
    "new_b = old_b - learning_rate * dl_db\n",
    "\n",
    "# NOTE: update loss based on new weight and new bias\n",
    "new_z1 = new_w * x1 + new_b\n",
    "\n",
    "# NOTE: if w & b are improved, the loss should have been lowered.\n",
    "new_w_loss = (new_w * x1 + b - y1) ** 2\n",
    "new_b_loss = (w * x1 + new_b - y1) ** 2\n",
    "\n",
    "# plot the loss function and loss of z1\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 10), dpi=50)\n",
    "\n",
    "# NOTE: plot loss v.s. w\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('loss / w')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(loss_ws, loss_wl, '-')\n",
    "plt.plot(w, loss, 'ro')\n",
    "plt.plot(new_w, new_w_loss, 'go')\n",
    "plt.arrow(w, loss, 0.1, 0.1 * dl_dw, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "\n",
    "# NOTE: plot loss v.s. b\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('loss / b')\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(loss_bs, loss_bl, '-')\n",
    "plt.plot(b, loss, 'go')\n",
    "plt.plot(new_b, new_b_loss, 'ro')\n",
    "plt.arrow(b, loss, 0.1, 0.1 * dl_db, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(10, 10), dpi=50)\n",
    "\n",
    "# NOTE: plot all the secret dots\n",
    "plt.plot(eigens, labels, '.')\n",
    "\n",
    "# NOTE: plot the stright line of old_w & old_b\n",
    "plt.plot([0.0, 5.0], [0.0 * old_w + old_b, 5.0 * old_w + old_b], 'ro-')\n",
    "\n",
    "# NOTE: plot the stright line of new_w & new_b\n",
    "plt.plot([0.0, 5.0], [0.0 * new_w + new_b, 5.0 * new_w + new_b], 'go-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Everything Together\n",
    "\n",
    "- change the learning rate to see the difference. why is it important?\n",
    "- change the step to see the difference. why is it important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change learning_rate to: -0.0001, 0.0, 0.1, 0.01, 0.001\n",
    "#       which one is better?\n",
    "learning_rate = 0.01\n",
    "\n",
    "# TODO: change num_steps to: 1, 10, 100, 1000,\n",
    "#       which one is better?\n",
    "num_steps = 10\n",
    "\n",
    "# TODO: change learnt_w or learnt_b to large value and see the results\n",
    "learnt_w, learnt_b = 0.6, 0.5\n",
    "\n",
    "losses = []\n",
    "\n",
    "for _ in xrange(num_steps):\n",
    "    # NOTE: pick a dot\n",
    "    random_i = np.random.randint(eigens.size)\n",
    "    \n",
    "    temp_x = eigens[random_i]\n",
    "    temp_y = labels[random_i]\n",
    "    \n",
    "    # NOTE: evaluate learnt_w & learnt_b on the picked dot\n",
    "    temp_z = learnt_w * temp_x + learnt_b\n",
    "\n",
    "    # NOTE: get the loss\n",
    "    temp_loss = (temp_z - temp_y) ** 2\n",
    "\n",
    "    # NOTE: record the loss\n",
    "    losses.append(temp_loss)\n",
    "\n",
    "    # NOTE: if temp_z is changed, what will happened to loss?\n",
    "    dl_dz = 2.0 * (temp_z - temp_y)\n",
    "\n",
    "    # NOTE: if w is changed, what will happened to loss?\n",
    "    dl_dw = dl_dz * temp_x\n",
    "\n",
    "    # NOTE: if b is changed, what will happened to loss?\n",
    "    dl_db = dl_dz\n",
    "\n",
    "    # NOTE: change learnt_w & learnt_b a little bit\n",
    "    learnt_w = learnt_w - learning_rate * dl_dw\n",
    "    learnt_b = learnt_b - learning_rate * dl_db\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 10), dpi=50)\n",
    "\n",
    "# NOTE: plot the learnt stright line\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(eigens, labels, '.')\n",
    "plt.plot([0.0, 5.0], [0.0 * w + b, 5.0 * w + b], 'r-')\n",
    "plt.plot([0.0, 5.0], [0.0 * learnt_w + learnt_b, 5.0 * learnt_w + learnt_b], 'b-')\n",
    "\n",
    "# NOTE: plot the loss of each iteration\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Linear Regression Model on KKTV Data Game 17.11 Dateset\n",
    "\n",
    "* train_eigens: (45728 users, 32 weeks * 28 slots)\n",
    "* train_labels: (45728 users, 28 slots of 33-th week)\n",
    "* valid_eigens: (11431 users, 32 weeks * 28 slots)\n",
    "* valid_labels: (11431 users, 28 slots of 33-th week)\n",
    "\n",
    "We want to train a model with (train_eigens, train_labels), then validate the performance of the model on (valid_eigens, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('./datasets/v0_eigens.npz')\n",
    "\n",
    "train_data_size = dataset['train_eigens'].shape[0]\n",
    "valid_data_size = train_data_size / 5\n",
    "train_data_size = train_data_size - valid_data_size\n",
    "\n",
    "indices = np.arange(train_data_size + valid_data_size)\n",
    "\n",
    "train_data = dataset['train_eigens'][indices[:train_data_size]]\n",
    "valid_data = dataset['train_eigens'][indices[train_data_size:]]\n",
    "\n",
    "train_eigens = train_data[:, :-28]\n",
    "train_labels = train_data[:, -28:]\n",
    "valid_eigens = valid_data[:, :-28]\n",
    "valid_labels = valid_data[:, -28:]\n",
    "\n",
    "print 'train_eigens.shape = {}'.format(train_eigens.shape)\n",
    "print 'train_labels.shape = {}'.format(train_labels.shape)\n",
    "print 'valid_eigens.shape = {}'.format(valid_eigens.shape)\n",
    "print 'valid_labels.shape = {}'.format(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTE: plot a cherry picked user\n",
    "train_user_0_eigens = train_eigens[3]\n",
    "train_user_0_labels = train_labels[3]\n",
    "\n",
    "train_user_0_eigens = train_user_0_eigens.reshape(-1, 28)\n",
    "train_user_0_labels = train_user_0_labels.reshape(-1, 28)\n",
    "\n",
    "gs = matplotlib.gridspec.GridSpec(2, 1, height_ratios=[32, 1])\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 10), dpi=50)\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(train_user_0_eigens, cmap='gray')\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(train_user_0_labels, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x W + b = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_0_eigens = train_user_0_eigens.reshape(1, -1)\n",
    "train_user_0_labels = train_user_0_labels.reshape(1, -1)\n",
    "\n",
    "random_weights = np.random.normal(size=(896, 28))\n",
    "random_biases = np.random.normal(size=(1, 28))\n",
    "\n",
    "gs = matplotlib.gridspec.GridSpec(1, 4, width_ratios=[16, 8, 4, 4])\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 10), dpi=50)\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(train_user_0_eigens, aspect=100.0, cmap='gray')\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(random_weights, aspect='auto', cmap='gray')\n",
    "plt.subplot(gs[2])\n",
    "plt.imshow(random_biases, aspect=20.0, cmap='gray')\n",
    "plt.subplot(gs[3])\n",
    "plt.imshow(train_user_0_labels, aspect=20.0, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "![](./assets/002_nn.png)\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "x_i = \\begin{bmatrix} x_{i, 0}, \\cdots , x_{i, 895} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "y_i = \\begin{bmatrix} y_{i, 0}, \\cdots , y_{i, 27} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "w = \\begin{pmatrix}w_{0,0} && \\cdots && w_{0,27} \\\\ \\vdots && \\ddots && \\vdots \\\\ w_{895,0} && \\cdots && w_{895,27} \\end{pmatrix}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "b = \\begin{bmatrix} b_{0}, \\cdots , b_{27} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "* $x_i$ : $user_i$ 's features (32 weeks playback log).\n",
    "* $y_i$ : $user_i$ 's label (33-th week playback log).\n",
    "* we want to find (train) good $w$ and $b$ so that for all users, $x_i \\cdot w + b = y_i $\n",
    "\n",
    "Expected result: AUC ~ 0.773074102425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 0.00001\n",
    "\n",
    "# NOTE: initialize w & b\n",
    "w = (np.random.rand(train_eigens.shape[1], 28) * 2.0 - 1.0) * 0.01\n",
    "b = np.zeros((1, 28))\n",
    "\n",
    "# NOTE: get the AUC before training\n",
    "valid_guesss = np.dot(valid_eigens, w) + b\n",
    "\n",
    "auc = util.auc(valid_guesss.flatten(), valid_labels.flatten())\n",
    "\n",
    "print 'auc[before]: {}'.format(auc)\n",
    "\n",
    "for step, eigens, labels in util.mini_batches(train_eigens, train_labels, batch_size, False):\n",
    "    # NOTE: tensor version of y = xw +b\n",
    "    y = np.dot(eigens, w) + b\n",
    "    \n",
    "    # NOTE: tensor version of loss\n",
    "    loss = np.sum(np.square(y - labels), axis=1)\n",
    "\n",
    "    if step % 5000 == 0:\n",
    "        print 'loss[{:>8}]: {}'.format(step, np.mean(loss))\n",
    "\n",
    "    # NOTE: tensor version of chain rule\n",
    "    dl_dy = 2.0 * (y - labels)\n",
    "    dl_dw = np.mean(np.dot(eigens.T, dl_dy), axis=0)\n",
    "    dl_db = np.mean(dl_dy, axis=0)\n",
    "    \n",
    "    # NOTE: update w & b\n",
    "    w = w - learning_rate * dl_dw\n",
    "    b = b - learning_rate * dl_db\n",
    "    \n",
    "    if step == 30000:\n",
    "        break\n",
    "\n",
    "# NOTE: get the AUC after training\n",
    "valid_guesss = np.dot(valid_eigens, w) + b\n",
    "\n",
    "auc = util.auc(valid_guesss.flatten(), valid_labels.flatten())\n",
    "\n",
    "print 'auc[after]: {}'.format(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inception = mpimg.imread('./datasets/inception.png')\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 10), dpi=50)\n",
    "plt.imshow(image_inception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hidden Layer\n",
    "\n",
    "![](./assets/003_nn.png)\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "W_1 \\cdot x + b_1 = h_1 \\\\\n",
    "W_2 \\cdot h_1 + b_2 = y\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 0.00001\n",
    "\n",
    "w1 = (np.random.rand(train_eigens.shape[1], 128) * 2.0 - 1.0) * 0.01\n",
    "b1 = np.zeros((1, 128))\n",
    "w2 = (np.random.rand(128, 28) * 2.0 - 1.0) * 0.01\n",
    "b2 = np.zeros((1, 28))\n",
    "\n",
    "y1 = np.dot(valid_eigens, w1) + b1\n",
    "y1[y1 < 0.0] = 0.0\n",
    "\n",
    "y2 = np.dot(y1, w2) + b2\n",
    "y2 = 1.0 / (1.0 + np.exp(-y2))\n",
    "\n",
    "valid_guesss = y2\n",
    "\n",
    "auc = util.auc(valid_guesss.flatten(), valid_labels.flatten())\n",
    "\n",
    "print 'auc[before]: {}'.format(auc)\n",
    "\n",
    "for step, eigens, labels in util.mini_batches(train_eigens, train_labels, batch_size, False):\n",
    "    # NOTE: 1st hidden layer\n",
    "    y1 = np.dot(eigens, w1) + b1\n",
    "    \n",
    "    # NOTE: relu\n",
    "    y1[y1 < 0.0] = 0.0\n",
    "\n",
    "    # NOTE: output layer\n",
    "    y2 = np.dot(y1, w2) + b2\n",
    "   \n",
    "    loss = np.sum(np.square(y2 - labels), axis=1)\n",
    "\n",
    "    if step % 5000 == 0:\n",
    "        print 'loss[{:>8}]: {}'.format(step, np.mean(loss))\n",
    "    \n",
    "    dl_dy2 = 2.0 * (y2 - labels)\n",
    "    dl_dw2 = np.mean(np.dot(y1.T, dl_dy2), axis=0)\n",
    "    dl_db2 = np.mean(dl_dy2, axis=0)\n",
    "\n",
    "    # NOTE: relu\n",
    "    dl_dy1 = np.dot(dl_dy2, w2.T) * (y1 > 0.0)\n",
    "    \n",
    "    # TODO: calculate dl_dw1\n",
    "    dl_dw1 = None\n",
    "    \n",
    "    # TODO: calculate dl_db1\n",
    "    dl_db1 = None\n",
    "    \n",
    "    w2 = w2 - learning_rate * dl_dw2\n",
    "    b2 = b2 - learning_rate * dl_db2\n",
    "    \n",
    "    # TODO: update w1 & b1\n",
    "    w1 = None\n",
    "    b1 = None\n",
    "    \n",
    "    if step == 30000:\n",
    "        break\n",
    "        \n",
    "y1 = np.dot(valid_eigens, w1) + b1\n",
    "y1[y1 < 0.0] = 0.0\n",
    "\n",
    "y2 = np.dot(y1, w2) + b2\n",
    "y2 = 1.0 / (1.0 + np.exp(-y2))\n",
    "\n",
    "valid_guesss = y2\n",
    "\n",
    "auc = util.auc(valid_guesss.flatten(), valid_labels.flatten())\n",
    "\n",
    "# NOTE: ~0.81523530014\n",
    "print 'auc[after]: {}'.format(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function: ReLU (Rectified Linear Unit)\n",
    "\n",
    "![](./assets/004_relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puzzle Time\n",
    "\n",
    "Consider the linear function $W \\cdot x + b = y$. If the best solution is to copy the 32-th week activities, what should $W$ & $b$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_0_eigens = train_eigens[3].reshape(1, -1)\n",
    "train_user_0_labels = train_labels[3].reshape(1, -1)\n",
    "\n",
    "w_get_32 = np.zeros((896, 28))\n",
    "b_get_32 = np.zeros((1, 28))\n",
    "\n",
    "# TODO: overwrite w_get_32 & b_get_32 so that the results always equal to 32-th week.\n",
    "\n",
    "z = np.matmul(train_user_0_eigens, w_get_32) + b_get_32\n",
    "\n",
    "print np.sum(z == train_user_0_eigens[0, -28:]) == 28\n",
    "\n",
    "gs = matplotlib.gridspec.GridSpec(2, 1, height_ratios=[896, 1])\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20, 20), dpi=50)\n",
    "plt.subplot(gs[0])\n",
    "plt.imshow(w_get_32, aspect='auto', cmap='gray')\n",
    "plt.subplot(gs[1])\n",
    "plt.imshow(b_get_32, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
